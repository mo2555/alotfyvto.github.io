import { MeshDetection, FacePose, Coord3D, PosePoint, PoseDetection, HandPoint, WristDetection, HandDetection } from '@geenee/bodytracking';
export { HandPoint, MeshDetection, PosePoint, WristDetection, WristLine } from '@geenee/bodytracking';
import { ProcParams, Processor, Size, ImageInput, Engine, EngineParams, VideoSource } from '@geenee/armature';

/** Parameters of {@link FaceProcessor} */
interface FaceParams extends ProcParams {
    /**
     * Evaluate transformation aligning reference face 3D model
     * with the measured one. Applying this transformation one can
     * align 3D object with the current pose (translation+rotation)
     * of the head. If the model's initial position is aligned with
     * the reference face, relative transformation will be preserved
     */
    transform?: boolean;
    /**
     * Evaluate metric 3D points - points within 3D space
     * of perspective camera located at the space origin
     * and pointed in the negative direction of the Z-axis.
     * These points can be used to apply texture face mask.
     */
    metric?: boolean;
    backproj?: boolean;
    /**
     * Evaluate with higher precision for lips, eyes and irises.
     * When enabled computation is slower, but contours of lips
     * are much more accurate and eye/iris detection is enabled.
     */
    highp?: boolean;
}
/** Face detection */
interface Face {
    /**
     * Basic face detection {@link MeshDetection | results}.
     * 2D pixel face landmarks - points in the screen coordinate space.
     * X and Y coordinates are normalized screen coordinates (scaled by
     * width and height of the input image), while the Z coordinate is
     * depth within orthographic projection space. These points can be
     * used for 2D face filters or when using orthographic projection.
     * Additionally, bounding box [x0, y0, x1, y1] and reliability score.
     */
    mesh?: MeshDetection;
    /**
     * Face pose - transformation matrix (translation+rotation+scale)
     * aligning reference face 3D model with the measured 3D face mesh.
     * Applying this transformation one can align 3D object with the
     * detected face. If the model's initial position is aligned with
     * the reference face, relative transformation will be preserved.
     */
    transform?: FacePose;
    /**
     * 3D metric points - points within 3D space of perspective camera
     * located at the space origin and pointed in the negative direction
     * of the Z-axis. These points can be used to apply texture face mask.
     */
    metric?: Coord3D[];
    backproj?: Coord3D[];
}
/** Results of {@link FaceProcessor} */
interface FaceResult {
    faces?: Face[];
}
/**
 * Face mesh processor
 *
 * Face mesh processor estimates 3D face landmarks, it detects and
 * tracks face mesh providing smooth, stable and accurate results.
 * Processor evaluates 2D pixel and 3D metric points as well as face
 * pose (translation+rotation+scale) aligning reference face model.
 * 2D pixel face landmarks - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space. These points can be
 * used for 2D face filters or when using orthographic projection.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used to apply texture face mask.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 * Face pose - transformation matrix (translation+rotation+scale)
 * aligning reference face 3D model with the measured 3D face mesh.
 * Applying this transformation one can align 3D object with the
 * detected face. If the model's initial position is aligned with
 * the reference face, relative transformation will be preserved.
 */
declare class FaceProcessor extends Processor<FaceResult, FaceParams> {
    /** Face mesh tracker (computation engine) */
    private faceTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /**
     * Initialize processor
     *
     * Prepares all resources required for face mesh tracking.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: FaceParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Face mesh processor detects and tracks faces.
     *
     * @param input - Image
     * @returns Face mesh or undefined
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<FaceResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Face mesh processor fixes FoV for more accurate estimation.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Face engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link FaceProcessor}. Straightforward wrapper,
 * instead of `new Engine(FaceProcessor, ...)` you can
 * use simpler `const engine = new FaceEngine(...})`.
 */
declare class FaceEngine extends Engine<FaceResult, FaceParams, FaceProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

/** Parameters of {@link PoseProcessor} */
interface PoseParams extends ProcParams {
    /**
     * Evaluate body segmentation mask
     *
     * Segmentation mask is a monochrome image having the same
     * size as input, where every pixel has value from 0 to 1
     * denoting the probability of it being a foreground pixel.
     */
    mask?: boolean;
}
/** Pose points */
type PosePoints = {
    [key in PosePointName]: PosePoint;
};
/** Name of pose point */
type PosePointName = typeof PosePointNames[number];
type PoseMask = NonNullable<PoseDetection["mask"]>;
type PoseDebug = NonNullable<PoseDetection["debug"]>;
/** Pose detection */
interface Pose {
    /**
     * List of pose {@link PosePoint | keypoints}
     *
     * 2D pixel coordinate - point in the screen coordinate space.
     * XY coordinates are normalized screen coordinates (scaled by
     * image width and height), while the Z coordinate is depth
     * in orthographic projection space, it has the same scale as X.
     * 3D metric coordinate - point within 3D space of perspective
     * camera located at the space origin and pointed in the negative
     * direction of the Z-axis. 3D & 2D points are perfectly aligned.
     */
    points: PosePoints;
    /** Reliability score, number between 0 and 1 */
    score: number;
    /**
     * Body segmentation mask
     *
     * Segmentation mask is a monochrome image, where every pixel has value
     * from 0 to 1 denoting the probability of it being a foreground pixel.
     * Body mask is provided for normalized rect region of original image.
     * Mask has a fixed size in pixels and should be scaled to image space.
     */
    mask?: PoseMask;
    debug?: PoseDebug;
}
/** Results of {@link PoseProcessor} */
interface PoseResult {
    /** Array of detected {@link Pose | poses} */
    poses?: Pose[];
}
/**
 * Pose processor
 *
 * Pose processor estimates 33 2D and 3D pose keypoints, it locates
 * the person / pose region-of-interest (ROI) and predicts the pose
 * keypoints providing smooth, stable and accurate pose estimation.
 * 2D pixel pose keypoints - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space, it has the same scale
 * as X coordinate (normalized by image width) and 0 is at the center
 * of hips. These points can be used for 2D pose overlays or when
 * using orthographic projection. Estimation of Z coordinate is not
 * very accurate and we recommend to use only XY for 2D effects.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used for 3D avatar overlays or
 * virtual try-on. Rigged and skinned models can be rendered on top
 * of the pose aligning skeleton/armature joints with 3D keypoints.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 */
declare class PoseProcessor extends Processor<PoseResult, PoseParams> {
    /** Pose tracker (computation engine) */
    private poseTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /** Constructor */
    constructor();
    /**
     * Initialize processor
     *
     * Prepares all resources required for pose estimation.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: PoseParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Pose processor detects pose and predicts pose keypoints.
     *
     * @param input - Image
     * @returns Pose estimation or undefined
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<PoseResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Pose processor fixes FoV for more accurate pose alignment.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Pose engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link PoseProcessor}. Straightforward wrapper,
 * instead of `new Engine(PoseProcessor, ...)` you can
 * use simpler `const engine = new PoseEngine(...})`.
 */
declare class PoseEngine extends Engine<PoseResult, PoseParams, PoseProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}
/** List of pose points */
declare const PosePointNames: readonly ["nose", "eyeInnerL", "eyeL", "eyeOutterL", "eyeInnerR", "eyeR", "eyeOutterR", "earL", "earR", "mouthL", "mouthR", "shoulderL", "shoulderR", "elbowL", "elbowR", "wristL", "wristR", "pinkyL", "pinkyR", "indexL", "indexR", "thumbL", "thumbR", "hipL", "hipR", "kneeL", "kneeR", "ankleL", "ankleR", "heelL", "heelR", "footIndexL", "footIndexR"];

/** Parameters of {@link HandProcessor} */
interface HandParams extends ProcParams {
    /**
     * Detect wrist
     *
     * Wrist detection result is the middle line
     * and two wrist edges provided as normalized
     * anchor points and unit direction vector.
     */
    wrist?: boolean;
}
/** Hand detection */
interface Hand {
    /**
     * List of hand {@link HandPoint | keypoints}
     *
     * 2D pixel coordinate - point in the screen coordinate space.
     * XY coordinates are normalized screen coordinates (scaled by
     * image width and height), while the Z coordinate is depth
     * in orthographic projection space, it has the same scale as X.
     * 3D metric coordinate - point within 3D space of perspective
     * camera located at the space origin and pointed in the negative
     * direction of the Z-axis. 3D & 2D points are perfectly aligned.
     */
    points: HandPoint[];
    /** Reliability score, number between 0 and 1 */
    score: number;
    /**
     * Classification score of handedness
     *
     * Number between -1 and 1 that represents handedness.
     * Negative value signals right hand, positive - left.
     * Bigger magnitude (absolute value) means more rebust
     * classification, the closer value is to zero - the
     * more ambiguous is distinction of the handedness.
     */
    handedness: number;
    /**
     * Wrist detection
     *
     * Wrist detection provides 3 lines in screen coordinate space.
     * Middle line defines 2D wrist center and unit direction vector
     * of the wrist. Two wrist edges are defined by 2D points at the
     * end of the wrist along transversal section through the center.
     */
    wrist: WristDetection;
    debug?: HandDetection["debug"];
}
/** Results of {@link HandProcessor} */
interface HandResult {
    /** Array of detected {@link Hand | hands} */
    hands?: Hand[];
}
/**
 * Hand processor
 *
 * Hand processor estimates 21 2D and 3D hand keypoints, it locates
 * the hand region-of-interest (ROI) and predicts the pose keypoints
 * providing smooth, stable and accurate pose estimation fot the hand.
 * 2D pixel hand keypoints - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space, it has the same scale
 * as X coordinate (normalized by image width). 2D points can be used
 * for 2D overlays, math analyzes, or when using orthographic camera.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used for 3D model overlays or
 * virtual try-on. Rigged and skinned models can be rendered on top
 * of the pose aligning skeleton/armature joints with 3D keypoints.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 * Additionally hand processor detects wrist 2D position and direction.
 * Wrist detection provides 3 lines in the screen coordinate space.
 * Middle line defines 2D wrist base/center point and unit direction
 * vector of the wrist. Two more lines define wrist edges by 2D screen
 * points at the end of the wrist along transversal section through
 * the base point and associated direction vectors. Wrist detection
 * provides for virtual try-on of accessories like watches and bands.
 */
declare class HandProcessor extends Processor<HandResult> {
    /** Hand tracker (computation engine) */
    private handTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /**
     * Initialize processor
     *
     * Prepares all resources required for pose estimation.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: ProcParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Pose processor detects hand and predicts keypoints.
     *
     * @param input - Image
     * @returns Pose estimation or undefined
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<HandResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Hand processor fixes FoV for more accurate hand alignment.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Hand engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link HandProcessor}. Straightforward wrapper,
 * instead of `new Engine(HandProcessor, ...)` you can
 * use simpler `const engine = new HandEngine(...})`.
 */
declare class HandEngine extends Engine<HandResult, ProcParams, HandProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

export { Face, FaceEngine, FaceParams, FaceProcessor, FaceResult, Hand, HandEngine, HandParams, HandProcessor, HandResult, Pose, PoseEngine, PoseParams, PosePointName, PosePoints, PoseProcessor, PoseResult };
